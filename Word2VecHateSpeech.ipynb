{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"word2vec_twitter_tokens.bin\"\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format(url, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings.most_similar('bitch', topn = 5)\n",
    "#embeddings.doesnt_match(['apple','banana','flower'])\n",
    "#embeddings.most_similar(positive = ['king','woman'], negative = ['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Tweet Classification\n",
      "0                 bird feather stand peta stand sanity              0\n",
      "1    wake di morning pocket fill xtra pro slide thr...              1\n",
      "2           trash snd star scrape ur team snd pro team              0\n",
      "3                           nt ud pussy yourself pussy              1\n",
      "4                                   life bitch dog day              1\n",
      "..                                                 ...            ...\n",
      "897  iu honorary student id cooler international st...              0\n",
      "898  girl colored people die holocaust picture holo...              0\n",
      "899                                        flappy bird              0\n",
      "900  arent stick garbage recycle im single fuck im ...              1\n",
      "901                                    post song guala              0\n",
      "\n",
      "[902 rows x 2 columns]\n",
      "                                                Tweet Classification\n",
      "0                    hoe lay puppy eye sayin stay wit              1\n",
      "1                               swap meet bind beaner              0\n",
      "2   beautiful fairy pool near glenbrittle isle sky...              0\n",
      "3                          bitch crib cook cheese egg              1\n",
      "4   morning birdminus  min thee clock breakfast sh...              0\n",
      "..                                                ...            ...\n",
      "95                          hoe bent worth lose sleep              1\n",
      "96                          mental asylum dalla texas              1\n",
      "97              ram dont wanna trash jeff fisher bout              0\n",
      "98  song truth wanna fuck bestfriend night onlyy b...              1\n",
      "99  run tree bird fly avoid erm target lack digest...              0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "colnames=['Tweet', 'Classification']\n",
    "train_csv = pd.read_csv('C:/Users/U713884/Documents/Nordakademie/WPM Einführung in die KI/OwnPartitioning/hatespeech_train100.csv', names=colnames, header=None, delimiter=';').astype('U')\n",
    "train_csv.head()\n",
    "test_csv = pd.read_csv('C:/Users/U713884/Documents/Nordakademie/WPM Einführung in die KI/OwnPartitioning/hatespeech_test.csv', names=colnames, header=None, delimiter=';').astype('U')\n",
    "test_csv.head()\n",
    "print (train_csv)\n",
    "print (test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 400)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectors_train = pd.DataFrame() \n",
    "for doc in train_csv['Tweet']:\n",
    "    temp = pd.DataFrame()  \n",
    "    for word in doc.split(' '):\n",
    "        try:\n",
    "            word_vec = embeddings[word] \n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean() \n",
    "    docs_vectors_train = docs_vectors_train.append(doc_vector, ignore_index = True) \n",
    "docs_vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_vectors_test = pd.DataFrame()\n",
    "for doc in test_csv['Tweet']:\n",
    "    temp = pd.DataFrame()\n",
    "    for word in doc.split(' '): \n",
    "        try:\n",
    "            word_vec = embeddings[word] \n",
    "            temp = temp.append(pd.Series(word_vec), ignore_index = True)\n",
    "        except:\n",
    "            pass\n",
    "    doc_vector = temp.mean()\n",
    "    docs_vectors_test = docs_vectors_test.append(doc_vector, ignore_index = True)\n",
    "docs_vectors_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(docs_vectors_train).sum().sum()\n",
    "pd.isnull(docs_vectors_test).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6    \\\n",
      "0    0.138487 -0.002912 -0.121982 -0.061652 -0.179469 -0.129264 -0.227865   \n",
      "1    0.128101  0.042784 -0.126150  0.080017  0.071535 -0.243294 -0.106714   \n",
      "2    0.152572  0.035148 -0.090621  0.069204  0.196073 -0.425802 -0.009700   \n",
      "3    0.106091  0.217428 -0.271630  0.013729 -0.303250 -0.149806  0.104003   \n",
      "4    0.258321  0.135977 -0.059272 -0.094361 -0.161200 -0.368803 -0.088933   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "897  0.076132  0.044501 -0.096838 -0.030460 -0.034590 -0.408548 -0.101186   \n",
      "898  0.183686  0.024167  0.054585  0.093391 -0.157928 -0.281188 -0.024889   \n",
      "899  0.209430  0.200209 -0.155619  0.258292  0.064989  0.052761 -0.230197   \n",
      "900  0.140382  0.041249 -0.020796 -0.049733  0.157028 -0.083795 -0.014951   \n",
      "901  0.235855  0.062413 -0.082691 -0.170068  0.151487 -0.259424  0.215690   \n",
      "\n",
      "          7         8         9    ...       390       391       392  \\\n",
      "0   -0.196397 -0.047069 -0.307181  ... -0.005176  0.020307  0.282899   \n",
      "1   -0.145713  0.006705 -0.092850  ...  0.004929 -0.004930  0.023449   \n",
      "2   -0.213637  0.017052 -0.031717  ... -0.140527 -0.028954 -0.066530   \n",
      "3   -0.184259 -0.046244 -0.093914  ...  0.324437  0.226918 -0.082652   \n",
      "4   -0.086482  0.104979  0.087777  ...  0.070628 -0.073156  0.100099   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "897 -0.091685 -0.237411  0.047750  ... -0.006002 -0.025863 -0.030556   \n",
      "898  0.006617 -0.062662  0.011110  ...  0.336376  0.028278  0.125164   \n",
      "899 -0.306395  0.430137  0.104461  ...  0.098025 -0.338996  0.015581   \n",
      "900 -0.085907 -0.021852  0.004311  ...  0.022667  0.026008  0.120839   \n",
      "901 -0.102140 -0.082282  0.099821  ... -0.021075 -0.245772 -0.039277   \n",
      "\n",
      "          393       394       395       396       397       398       399  \n",
      "0   -0.115352  0.286320  0.016553  0.317832  0.013245  0.451480  0.050031  \n",
      "1    0.075357  0.084899  0.178685  0.117885 -0.025239  0.001537 -0.133108  \n",
      "2   -0.018389  0.179127  0.194193  0.098640  0.055005  0.013902  0.084717  \n",
      "3    0.017559  0.009225  0.347022 -0.072544  0.118140  0.214915  0.027661  \n",
      "4    0.099365  0.242746  0.121660  0.373411 -0.059116  0.173977  0.043605  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "897  0.033705  0.174949 -0.076729  0.148122 -0.110186  0.178446  0.015813  \n",
      "898  0.021987  0.042812  0.191038  0.051776  0.083799  0.317250  0.028033  \n",
      "899  0.053732  0.778945  0.171636  0.455887 -0.002054  0.527126  0.111163  \n",
      "900 -0.114614  0.338958  0.205843 -0.022221  0.171377  0.290259 -0.149010  \n",
      "901  0.182307  0.186300 -0.068862 -0.000986 -0.122260  0.082184  0.153635  \n",
      "\n",
      "[902 rows x 400 columns]\n"
     ]
    }
   ],
   "source": [
    "print (docs_vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors_train['Classification'] = train_csv['Classification']\n",
    "docs_vectors_train = docs_vectors_train.dropna()\n",
    "\n",
    "docs_vectors_test['Classification'] = test_csv['Classification']\n",
    "docs_vectors_test = docs_vectors_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900, 400), (900,), (99, 400), (99,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "docs_vectors_train.drop('Classification', axis = 1)\n",
    "docs_vectors_test.drop('Classification', axis = 1)\n",
    "\n",
    "train_x = docs_vectors_train.drop('Classification', axis = 1)\n",
    "train_y = docs_vectors_train['Classification']\n",
    "test_x = docs_vectors_test.drop('Classification', axis = 1)\n",
    "test_y = docs_vectors_test['Classification']\n",
    "\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.869\n",
      "confusion matrix:\n",
      "[[44  4]\n",
      " [ 9 42]]\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=900, learning_rate=0.1, random_state = 1)\n",
    "model.fit(train_x, train_y)\n",
    "test_pred = model.predict(test_x)\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy: %0.3f\" % metrics.accuracy_score(test_y, test_pred))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
